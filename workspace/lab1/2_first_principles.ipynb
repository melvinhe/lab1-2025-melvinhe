{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Analyzing Deep Neural Networks\n",
    "In this notebook, we will recreate the functions that PyTorch provides for the\n",
    "individual layers in our network using primtive Python code. The goal of this\n",
    "notebook is to understand what happens in a network underneath the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from loaders import *\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import inspect\n",
    "from math import prod\n",
    "\n",
    "%matplotlib inline\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing PyTorch functions\n",
    "First, we will implement the functions that PyTorch provides for the individual\n",
    "layers in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1: What is your implementation of the conv function?\n",
      "\tdef conv(x, W, b):\n",
      "    # DO NOT USE ANY LIBRARY CONVOLUTION FUNCTIONS. WRITE YOUR OWN LOOP NEST.\n",
      "    if np.ndim(x) == 2:\n",
      "        x = np.expand_dims(x, axis = 0)\n",
      "    assert np.ndim(x) == 3\n",
      "    C_in, M, N = x.shape\n",
      "    C_out, C_t, Kx, Ky = W.shape\n",
      "    assert C_in == C_t, \"activation and weight dimension mismatch!\"\n",
      "    # Assume this padding\n",
      "    x_padded = np.pad(x,((0,0),(Kx//2,Kx//2),(Ky//2,Ky//2)))\n",
      "\n",
      "    # Your code here\n",
      "    out = np.zeros((C_out, M, N))\n",
      "    for c in range(C_out):\n",
      "        for i in range(M):\n",
      "            for j in range(N):\n",
      "                out[c, i, j] = np.sum(x_padded[:, i:i+Kx, j:j+Ky] * W[c, :, :, :]) + b[c]\n",
      "    return out\n",
      "\n",
      "2.1: What is your implementation of the fc function?\n",
      "\tdef fc(x, W, b):\n",
      "    # DO NOT USE ANY LIBRARY MATRIX MULTIPLICATION FUNCTIONS. WRITE YOUR OWN LOOP NEST.\n",
      "    # Your code here\n",
      "    assert x.shape[0] == W.shape[1], \"Input and weight dimension mismatch!\"\n",
      "    out = np.zeros(W.shape[0])\n",
      "    for i in range(W.shape[0]):\n",
      "        sum_val = 0\n",
      "        for j in range(W.shape[1]):\n",
      "            sum_val += x[j] * W[i, j]\n",
      "        out[i] = sum_val + b[i]\n",
      "    return out\n",
      "\n",
      "2.1: What is your implementation of the relu function?\n",
      "\tdef relu(x):\n",
      "    # Your code here\n",
      "    return np.maximum(0, x)\n",
      "\n",
      "2.1: What is your implementation of the pool2 function?\n",
      "\tdef pool2(x, dh, dw):\n",
      "    # DO NOT USE ANY LIBRARY POOLING FUNCTIONS. WRITE YOUR OWN LOOP NEST.\n",
      "    # Your code here\n",
      "    C, H, W = x.shape\n",
      "    H_out = H // dh\n",
      "    W_out = W // dw\n",
      "    out = np.zeros((C, H_out, W_out))\n",
      "    for c in range(C):\n",
      "        for i in range(H_out):\n",
      "            for j in range(W_out):\n",
      "                out[c, i, j] = np.max(x[c, i*dh:(i+1)*dh, j*dw:(j+1)*dw])\n",
      "    return out\n",
      "\n",
      "2.1: What is your implementation of the flatten function?\n",
      "\tdef flatten(x):\n",
      "    # Your code here\n",
      "    return x.flatten()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def conv(x, W, b):\n",
    "    # DO NOT USE ANY LIBRARY CONVOLUTION FUNCTIONS. WRITE YOUR OWN LOOP NEST.\n",
    "    if np.ndim(x) == 2:\n",
    "        x = np.expand_dims(x, axis = 0)\n",
    "    assert np.ndim(x) == 3\n",
    "    C_in, M, N = x.shape\n",
    "    C_out, C_t, Kx, Ky = W.shape\n",
    "    assert C_in == C_t, \"activation and weight dimension mismatch!\"\n",
    "    # Assume this padding\n",
    "    x_padded = np.pad(x,((0,0),(Kx//2,Kx//2),(Ky//2,Ky//2)))\n",
    "\n",
    "    # Your code here\n",
    "    out = np.zeros((C_out, M, N))\n",
    "    for c in range(C_out):\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                out[c, i, j] = np.sum(x_padded[:, i:i+Kx, j:j+Ky] * W[c, :, :, :]) + b[c]\n",
    "    return out\n",
    "    \n",
    "conv_test_x = np.random.rand(3, 5, 5)\n",
    "conv_test_W = np.random.rand(2, 3, 3, 3)\n",
    "conv_test_b = np.random.rand(2)\n",
    "conv_test_y = conv(conv_test_x, conv_test_W, conv_test_b)\n",
    "assert np.allclose(conv_test_y, conv_test_y), \"Convolution function is incorrect\"\n",
    "    \n",
    "\n",
    "def fc(x, W, b):\n",
    "    # DO NOT USE ANY LIBRARY MATRIX MULTIPLICATION FUNCTIONS. WRITE YOUR OWN LOOP NEST.\n",
    "    # Your code here\n",
    "    assert x.shape[0] == W.shape[1], \"Input and weight dimension mismatch!\"\n",
    "    out = np.zeros(W.shape[0])\n",
    "    for i in range(W.shape[0]):\n",
    "        sum_val = 0\n",
    "        for j in range(W.shape[1]):\n",
    "            sum_val += x[j] * W[i, j]\n",
    "        out[i] = sum_val + b[i]\n",
    "    return out\n",
    "\n",
    "fc_test_x = np.random.rand(5)\n",
    "fc_test_W = np.random.rand(3, 5)\n",
    "fc_test_b = np.random.rand(3)\n",
    "fc_test_y = fc(fc_test_x, fc_test_W, fc_test_b)\n",
    "assert np.allclose(fc_test_y, fc_test_y), \"Fully connected function is incorrect\"\n",
    "\n",
    "def relu(x):\n",
    "    # Your code here\n",
    "    return np.maximum(0, x)\n",
    "    \n",
    "relu_test_x = np.random.rand(5, 5)\n",
    "relu_test_y = relu(relu_test_x)\n",
    "assert np.allclose(relu_test_y, relu_test_y), \"ReLU function is incorrect\"\n",
    "\n",
    "\n",
    "def pool2(x, dh, dw):\n",
    "    # DO NOT USE ANY LIBRARY POOLING FUNCTIONS. WRITE YOUR OWN LOOP NEST.\n",
    "    # Your code here\n",
    "    C, H, W = x.shape\n",
    "    H_out = H // dh\n",
    "    W_out = W // dw\n",
    "    out = np.zeros((C, H_out, W_out))\n",
    "    for c in range(C):\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                out[c, i, j] = np.max(x[c, i*dh:(i+1)*dh, j*dw:(j+1)*dw])\n",
    "    return out\n",
    "    \n",
    "pool2_test_x = np.random.rand(3, 4, 4)\n",
    "pool2_test_y = pool2(pool2_test_x, 2, 2)\n",
    "assert np.allclose(pool2_test_y, pool2_test_y), \"Pooling function is incorrect\"\n",
    "\n",
    "def flatten(x):\n",
    "    # Your code here\n",
    "    return x.flatten()\n",
    "    \n",
    "flatten_test_x = np.random.rand(3, 4, 4)\n",
    "flatten_test_y = flatten(flatten_test_x)\n",
    "assert np.allclose(flatten_test_y, flatten_test_y), \"Flatten function is incorrect\"\n",
    "\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the conv function?\",\n",
    "    answer= str(inspect.getsource(conv)),\n",
    "    required_type=str\n",
    ")\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the fc function?\",\n",
    "    answer= str(inspect.getsource(fc)),\n",
    "    required_type=str\n",
    ")\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the relu function?\",\n",
    "    answer= str(inspect.getsource(relu)),\n",
    "    required_type=str\n",
    ")\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the pool2 function?\",\n",
    "    answer= str(inspect.getsource(pool2)),\n",
    "    required_type=str\n",
    ")\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the flatten function?\",\n",
    "    answer= str(inspect.getsource(flatten)),\n",
    "    required_type=str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test inference using the weights of our trained network and our own\n",
    "implementation of the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 974/1000 (0.97)\n",
      "Baseline Accuracy: 974/1000 (0.97)\n",
      "2.2: What is the accuracy of your implementation?\n",
      "\t974\n",
      "The accuracy of the implementation is the same as the baseline accuracy. Your implementation is correct.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the previous section\n",
    "PATH = './my_mnist_net.pth'\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, padding = 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, padding = 2)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH, weights_only=False))\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "images_array = torch.zeros((10000,28,28))\n",
    "labels_array = torch.zeros(10000)\n",
    "for i, data in enumerate(torch.utils.data.DataLoader(testset, batch_size=1), 0):\n",
    "    image, label = data\n",
    "    images_array[i,:,:] = image\n",
    "    labels_array[i] = label\n",
    "images_array = images_array.numpy()\n",
    "labels_array = labels_array.numpy()\n",
    "labels_array = labels_array.astype(int)\n",
    "\n",
    "def run_inference(image, net):\n",
    "    # Your code for defining the correct network topology here, using the saved model parameters.\n",
    "    # The first layer is filled in for you.\n",
    "    l1 = conv(image, net.conv1.weight.detach().numpy(), net.conv1.bias.detach().numpy())\n",
    "    #Your Answer Here\n",
    "    l2 = relu(l1)\n",
    "    l3 = pool2(l2, 2, 2)\n",
    "\n",
    "    l4 = conv(l3, net.conv2.weight.detach().numpy(), net.conv2.bias.detach().numpy())\n",
    "    l5 = relu(l4)\n",
    "    l6 = pool2(l5, 2, 2)\n",
    "\n",
    "    l7 = fc(l6.flatten(), net.fc1.weight.detach().numpy(), net.fc1.bias.detach().numpy())\n",
    "    l8 = relu(l7)\n",
    "    l9 = fc(l8, net.fc2.weight.detach().numpy(), net.fc2.bias.detach().numpy())\n",
    "\n",
    "    output_class = np.argmax(l9)\n",
    "    return output_class\n",
    "\n",
    "def run_inference_net(image, net):\n",
    "    with torch.no_grad():\n",
    "        image = torch.tensor(image).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "        output = net(image)\n",
    "        output_class = torch.argmax(output, dim=1).item()\n",
    "    return output_class\n",
    "    \n",
    "correct, correct_baseline, total = 0, 0, 1000\n",
    "for n in tqdm(range(min(total, int(images_array.shape[0])))):\n",
    "    inference = run_inference(images_array[n], net)\n",
    "    if labels_array[n] == inference:\n",
    "        correct += 1\n",
    "    inference = run_inference_net(images_array[n], net)\n",
    "    if labels_array[n] == inference:\n",
    "        correct_baseline += 1\n",
    "\n",
    "print(f\"Accuracy: {correct}/{total} ({float(correct)/total:.2g})\")\n",
    "print(f\"Baseline Accuracy: {correct_baseline}/{total} ({float(correct_baseline)/total:.2g})\")\n",
    "\n",
    "answer(\n",
    "    \"2.2\",\n",
    "    subquestion=\"What is the accuracy of your implementation?\",\n",
    "    answer= 974,\n",
    "    required_type=int\n",
    ")\n",
    "if correct != correct_baseline:\n",
    "    print(f'The accuracy of the implementation is not the same as the baseline accuracy. There is likely a bug in the implementation.')\n",
    "else:\n",
    "    print(f'The accuracy of the implementation is the same as the baseline accuracy. Your implementation is correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to implement the layers of the network, let's analyze some\n",
    "important properties of the network.\n",
    "\n",
    "Now we'll load the network from the previous section and analyze it. Fill a description of the network by filling in the code below. The first and last layers are filled in for you. Include:\n",
    "1. Convolutional layers: Specify a 4-tuple `(weight_width, weight_height, input_channels, filter_count)` (remember that there is one filter for each output channel)\n",
    "2. Fully connected layers: Specify a 2-tuple `(num_output_nodes, num_input_nodes)`\n",
    "3. Pool layer: Specify `(x_window, y_window, x_stride, y_stride)`\n",
    "4. ReLU: No parameters to specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3: What is the layer type of layer 1?\n",
      "\tconv\n",
      "2.3: What are the parameters of layer 1?\n",
      "\t(5, 5, 1, 4)\n",
      "2.3: What is the layer type of layer 2?\n",
      "\trelu\n",
      "2.3: What are the parameters of layer 2?\n",
      "\t()\n",
      "2.3: What is the layer type of layer 3?\n",
      "\tpool\n",
      "2.3: What are the parameters of layer 3?\n",
      "\t(2, 2, 2, 2)\n",
      "2.3: What is the layer type of layer 4?\n",
      "\tconv\n",
      "2.3: What are the parameters of layer 4?\n",
      "\t(5, 5, 4, 8)\n",
      "2.3: What is the layer type of layer 5?\n",
      "\trelu\n",
      "2.3: What are the parameters of layer 5?\n",
      "\t()\n",
      "2.3: What is the layer type of layer 6?\n",
      "\tpool\n",
      "2.3: What are the parameters of layer 6?\n",
      "\t(2, 2, 2, 2)\n",
      "2.3: What is the layer type of layer 7?\n",
      "\tfc\n",
      "2.3: What are the parameters of layer 7?\n",
      "\t(256, 392)\n",
      "2.3: What is the layer type of layer 8?\n",
      "\trelu\n",
      "2.3: What are the parameters of layer 8?\n",
      "\t()\n",
      "2.3: What is the layer type of layer 9?\n",
      "\tfc\n",
      "2.3: What are the parameters of layer 9?\n",
      "\t(10, 256)\n",
      "2.3: What is the size of the input to the network?\n",
      "\t(1, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "def conv_description(weight_width, weight_height, input_channels, filter_count):\n",
    "    return \"conv\", (weight_width, weight_height, input_channels, filter_count)\n",
    "\n",
    "def relu_description():\n",
    "    return \"relu\", ()\n",
    "\n",
    "def pool_description(x_window, y_window, x_stride, y_stride):\n",
    "    return \"pool\", (x_window, y_window, x_stride, y_stride)\n",
    "\n",
    "def fc_description(num_output_nodes, num_input_nodes):\n",
    "    return \"fc\", (num_output_nodes, num_input_nodes)\n",
    "\n",
    "layer_type = [\n",
    "    conv_description(5, 5, 1, 4),\n",
    "    #Your Answer Here\n",
    "    relu_description(),\n",
    "    pool_description(2, 2, 2, 2),\n",
    "    conv_description(5, 5, 4, 8),\n",
    "    relu_description(),\n",
    "    pool_description(2, 2, 2, 2),\n",
    "    fc_description(256, 8 * 7 * 7),\n",
    "    relu_description(), \n",
    "    fc_description(10, 256)\n",
    "]\n",
    "\n",
    "network_input_batch = 1\n",
    "network_input_width = 28\n",
    "network_input_height = 28\n",
    "network_input_channels = 1\n",
    "\n",
    "network_input_size = (\n",
    "    network_input_batch,\n",
    "    network_input_channels,\n",
    "    network_input_width,\n",
    "    network_input_height\n",
    ")\n",
    "\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    answer( \n",
    "        question='2.3',\n",
    "        subquestion=f'What is the layer type of layer {index + 1}?',\n",
    "        answer= name,  #Do not Change this Line \n",
    "        required_type=str,\n",
    "    )\n",
    "    answer( \n",
    "        question='2.3',\n",
    "        subquestion=f'What are the parameters of layer {index + 1}?',\n",
    "        answer= param,  #Do not Change this Line\n",
    "        required_type=tuple,\n",
    "    )\n",
    "    \n",
    "answer(\n",
    "    question='2.3',\n",
    "    subquestion=f'What is the size of the input to the network?',\n",
    "    answer= network_input_size,  #Do not Change this Line\n",
    "    required_type=tuple,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of finding the layer input sizes is simply by inspection. Since the inputs of a subsequent layer are the outputs of a previous layer, we can also compute the size of these outputs based on the inputs sizes and weight parameters. Complete the `get_output_size` function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4: What is the output size of layer 1?\n",
      "\t(1, 1, 28, 4)\n",
      "2.4: What is the output size of layer 2?\n",
      "\t(1, 1, 28, 4)\n",
      "2.4: What is the output size of layer 3?\n",
      "\t(1, 0, 14, 4)\n",
      "2.4: What is the output size of layer 4?\n",
      "\t(1, 0, 14, 8)\n",
      "2.4: What is the output size of layer 5?\n",
      "\t(1, 0, 14, 8)\n",
      "2.4: What is the output size of layer 6?\n",
      "\t(1, 0, 7, 8)\n",
      "2.4: What is the output size of layer 7?\n",
      "\t(1, 1, 1, 256)\n",
      "2.4: What is the output size of layer 8?\n",
      "\t(1, 1, 1, 256)\n",
      "2.4: What is the output size of layer 9?\n",
      "\t(1, 1, 1, 10)\n",
      "Warning! There is a bug in your answer. Expected 11106 but got 746.\n"
     ]
    }
   ],
   "source": [
    "def get_output_size(input_sz, layer_type, layer_param):\n",
    "    # Return format: (batch_size, width, height, channels)\n",
    "    input_batch, input_width, input_height, input_channels = input_sz[0], input_sz[1], input_sz[2], input_sz[3]\n",
    "    # ReLU return is filled for you.\n",
    "    if layer_type == 'conv':\n",
    "        weight_width, weight_height, input_channels, filter_count = layer_param\n",
    "        stride = 1  # You may assume stride = 1\n",
    "        padding = 2  # You may assume padding = 2\n",
    "\n",
    "        # Your code here   \n",
    "        output_width = (input_width - weight_width + 2 * padding) // stride + 1\n",
    "        output_height = (input_height - weight_height + 2 * padding) // stride + 1\n",
    "        output_channels = filter_count\n",
    "        output_batch = input_batch\n",
    "\n",
    "        return (output_batch, output_width, output_height, output_channels)\n",
    "    \n",
    "    elif layer_type == 'pool':\n",
    "        x_window, y_window, x_stride, y_stride = layer_param\n",
    "\n",
    "        # Your code here\n",
    "        output_width = input_width // x_stride\n",
    "        output_height = input_height // y_stride\n",
    "        output_channels = input_channels\n",
    "        output_batch = input_batch\n",
    "\n",
    "    elif layer_type == 'fc':\n",
    "        num_output_nodes, num_input_nodes = layer_param\n",
    "\n",
    "        # Your code here\n",
    "        output_width = 1\n",
    "        output_height = 1\n",
    "        output_channels = num_output_nodes\n",
    "        output_batch = input_batch\n",
    "    elif layer_type == 'relu':\n",
    "        output_batch = input_batch\n",
    "        output_width = input_width\n",
    "        output_height = input_height\n",
    "        output_channels = input_channels\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "    return (output_batch, output_width, output_height, output_channels)\n",
    "\n",
    "in_sz = network_input_size\n",
    "sum_size = 0\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    out_sz = get_output_size(in_sz, name, param)\n",
    "    sum_size += np.prod(out_sz)\n",
    "    answer( \n",
    "        question='2.4',\n",
    "        subquestion=f'What is the output size of layer {index + 1}?',\n",
    "        answer= out_sz,  #Do not change this line\n",
    "        required_type=tuple,\n",
    "    )\n",
    "    in_sz = out_sz\n",
    "\n",
    "expected = 11106\n",
    "if sum_size != 11106:\n",
    "    print(f'Warning! There is a bug in your answer. Expected {expected} but got {sum_size}.')\n",
    "else:\n",
    "    print(f'Total number of outputs is correct. Good job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, complete the `get_weight_size` and function to calculate the number of\n",
    "weights required in each layer and the memory required for storing the weights.\n",
    "You may ignore the memory required for storing biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_width' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m sum_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, (name, param) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layer_type):\n\u001b[0;32m---> 34\u001b[0m     n_weights \u001b[38;5;241m=\u001b[39m \u001b[43mget_num_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     sum_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_weights\n\u001b[1;32m     36\u001b[0m     answer( \n\u001b[1;32m     37\u001b[0m         question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.5\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     38\u001b[0m         subquestion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHow many weights are there in layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     39\u001b[0m         answer\u001b[38;5;241m=\u001b[39m n_weights,  \u001b[38;5;66;03m# Do not change this line\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         required_type\u001b[38;5;241m=\u001b[39mNumber,\n\u001b[1;32m     41\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m, in \u001b[0;36mget_num_weights\u001b[0;34m(layer_type, layer_param)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Your code here\u001b[39;00m\n\u001b[1;32m      7\u001b[0m stride, padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 8\u001b[0m output_width \u001b[38;5;241m=\u001b[39m ((\u001b[43minput_width\u001b[49m \u001b[38;5;241m-\u001b[39m weight_width \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m padding) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m stride) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m output_height \u001b[38;5;241m=\u001b[39m ((input_height \u001b[38;5;241m-\u001b[39m weight_height \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m padding) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m stride) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     10\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m filter_count\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_width' is not defined"
     ]
    }
   ],
   "source": [
    "def get_num_weights(layer_type, layer_param):\n",
    "    # Return format: number_of_weights\n",
    "    # ReLU return is filled for you.\n",
    "    if layer_type == 'conv':\n",
    "        weight_width, weight_height, input_channels, filter_count = layer_param\n",
    "        # Your code here\n",
    "    elif layer_type == 'pool':\n",
    "        # Your code here\n",
    "    elif layer_type == 'fc':\n",
    "        num_output_nodes, num_input_nodes = layer_param\n",
    "        # Your code here\n",
    "    elif layer_type == 'relu':\n",
    "        number_of_weights = 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "    return number_of_weights\n",
    "\n",
    "sum_size = 0\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    n_weights = get_num_weights(name, param)\n",
    "    sum_size += n_weights\n",
    "    answer( \n",
    "        question='2.5',\n",
    "        subquestion=f'How many weights are there in layer {index + 1}?',\n",
    "        answer= n_weights,  # Do not change this line\n",
    "        required_type=Number,\n",
    "    )\n",
    "    \n",
    "expected = 103812\n",
    "if sum_size != 103812:\n",
    "    print(f'Warning! There is a bug in your answer. Expected {expected} but got {sum_size}.')\n",
    "else:\n",
    "    print(f'Total number of weights is correct. Good job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the number of multiplications required per _batch_. Multiplications by\n",
    "zero padding in convolutional layers should still be counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_mults(input_sz, layer_type, layer_param):\n",
    "    # Return format: number_of_multiplications\n",
    "    # ReLU return is filled for you.\n",
    "    input_batch, input_width, input_height, input_channels = input_sz[0], input_sz[1], input_sz[2], input_sz[3]\n",
    "    output_batch, output_width, output_height, output_channels = get_output_size(input_sz, layer_type, layer_param)\n",
    "    if layer_type == 'conv':\n",
    "        weight_width, weight_height, input_channels, filter_count = layer_param\n",
    "        num_mult = \\\n",
    "            # Your code here\n",
    "    elif layer_type == 'pool':\n",
    "        num_mult = \\\n",
    "            # Your code here\n",
    "    elif layer_type == 'fc':\n",
    "        num_output_nodes, num_input_nodes = layer_param\n",
    "        num_mult = \\\n",
    "            # Your code here\n",
    "    elif layer_type == 'relu':\n",
    "        num_mult = \\\n",
    "            0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "    return num_mult\n",
    "\n",
    "sum_size = 0\n",
    "in_sz = network_input_size\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    n_mult = get_num_mults(in_sz, name, param)\n",
    "    in_sz = get_output_size(in_sz, name, param)\n",
    "    sum_size += n_mult\n",
    "    answer( \n",
    "        question='2.6',\n",
    "        subquestion=f'How many multiplications are there in layer {index + 1}?',\n",
    "        answer= n_mult,\n",
    "        required_type=Number,\n",
    "    )\n",
    "\n",
    "expected = 338112.0\n",
    "if sum_size != 338112.0:\n",
    "    print(f'Warning! There is a bug in your answer. Expected {expected} but got {sum_size}.')\n",
    "else:\n",
    "    print(f'Total number of multiplications is correct. Good job!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compute_intensity(input_sz, layer_type, layer_param):\n",
    "    # Assume inputs, weights, and outputs are all read/written from main memory.\n",
    "    # Assume one multiply = one compute\n",
    "    # How many computations are done per value read/written?\n",
    "     return 1\n",
    "\n",
    "# Your code here\n",
    "\n",
    "sum_size = 0\n",
    "in_sz = network_input_size\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    n_mult = get_compute_intensity(in_sz, name, param)\n",
    "    in_sz = get_output_size(in_sz, name, param)\n",
    "    sum_size += n_mult\n",
    "    answer( \n",
    "        question='2.7',\n",
    "        subquestion=f'what is the compute intensity of layer {index + 1}?',\n",
    "        answer= n_mult,\n",
    "        required_type=Number,\n",
    "    )\n",
    "\n",
    "expected = 71.14813864089903\n",
    "if abs(sum_size - expected) / expected > 0.01:\n",
    "    print(f'Warning! There is a bug in your answer. Expected {expected} but got {sum_size}.')\n",
    "else:\n",
    "    print(f'Total compute intensity is correct. Good job!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
